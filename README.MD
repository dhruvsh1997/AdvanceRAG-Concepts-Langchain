# üöÄ Advanced RAG Experiments Repository

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://python.langchain.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Contributions](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](CONTRIBUTING.md)

> A comprehensive collection of advanced Retrieval-Augmented Generation (RAG) implementations, exploring cutting-edge techniques for document understanding, multimodal processing, and intelligent information retrieval.

---

## üìö Table of Contents

- [üéØ Overview](#overview)
- [üîß RAG Implementations](#rag-implementations)
  - [RAG with Metadata & Document Layout Analysis](#1-rag-with-metadata--document-layout-analysis)
  - [MultiModal RAG with CLIP Image Embedding](#2-multimodal-rag-with-clip-image-embedding)
  - [Speculative RAG with News Outlet Summary](#3-speculative-rag-with-news-outlet-summary)
  - [Corrective RAG](#4-corrective-rag)
  - [Self-RAG](#5-self-rag)
  - [Agentic RAG](#6-agentic-rag)
  - [Adaptive RAG: Smart Retrieval from DOC or LLM](#7-adaptive-rag-smart-retrieval-from-doc-or-llm)
  - [Fusion RAG: MultiSource-MultiRetrieval-MultiLingual](#8-fusion-rag-multisource-multiretrieval-multilingual)
  - [REFEED (Retrieval Feedback) HR Policy Fine-Tuning Without Training](#9-refeed-retrieval-feedback-hr-policy-fine-tuning-without-training)
- [üõ†Ô∏è Installation](#installation)
- [üìä Performance Comparison](#performance-comparison)
- [üöÄ Quick Start](#quick-start)
- [üìà Future Roadmap](#future-roadmap)
- [ü§ù Contributing](#contributing)
- [üìö Resources & References](#resources--references)
- [üìÑ License](#license)
- [üôè Acknowledgments](#acknowledgments)
- [üìä Repository Statistics](#repository-statistics)

---

## üéØ Overview

This repository showcases **nine advanced RAG implementations** that push the boundaries of traditional retrieval-augmented generation. Each implementation tackles specific challenges in document understanding, multimodal processing, and intelligent information retrieval.

### üåü Key Features

- **Advanced Document Processing**: Layout analysis, metadata extraction, and multi-column handling
- **Multimodal Understanding**: Combined text and image processing with CLIP embeddings
- **Self-Improving Systems**: Corrective and self-reflective RAG architectures
- **Agentic Workflows**: Graph-based RAG with integrated evaluation metrics
- **Production-Ready**: Robust error handling and optimization techniques
- **Adaptive Retrieval**: Dynamic strategy selection for optimal performance
- **Multisource Retrieval**: Combining multiple data sources and retrieval methods
- **Feedback-Driven Fine-Tuning**: Enhancing performance without traditional training

---

## üîß RAG Implementations

### 1. RAG with Metadata & Document Layout Analysis

**üéØ Purpose**: Intelligent document parsing with layout understanding and metadata preservation

```mermaid
graph TD
    A[PDF Document] --> B[Document Layout Analysis]
    B --> C[Column Detection]
    B --> D[Image Extraction]
    B --> E[Table Extraction]
    C --> F[Text Chunking with Metadata]
    D --> G[Image Processing]
    E --> H[Table Processing]
    F --> I[Vector Embedding]
    G --> I
    H --> I
    I --> J[FAISS Vector Store]
    J --> K[MMR Retrieval]
    K --> L[Context with Metadata]
    L --> M[LLM Generation]
    M --> N[Response with Source Tracing]
```

**üîë Key Features**:
- **Two-column detection** for academic papers
- **Metadata preservation** (page numbers, chunk IDs)
- **Multi-asset extraction** (images, tables, text)
- **Source traceability** in responses

**üìä Technical Stack**:
- `pdfplumber` for text extraction
- `PyMuPDF` for image extraction
- `HuggingFace Embeddings` for vectorization
- `FAISS` for similarity search
- `Groq LLaMA` for generation

### 2. MultiModal RAG with CLIP Image Embedding

**üéØ Purpose**: Unified text and image understanding using CLIP embeddings

```mermaid
graph TD
    A[PDF Document] --> B[Text Extraction]
    A --> C[Image Extraction]
    B --> D[Text Chunking]
    C --> E[Image Processing]
    D --> F[CLIP Text Encoder]
    E --> G[CLIP Image Encoder]
    F --> H[Unified Vector Space]
    G --> H
    H --> I[FAISS Index]
    I --> J[Multimodal Retrieval]
    J --> K[Text + Image Context]
    K --> L[GPT-4 Vision]
    L --> M[Multimodal Response]
```

**üîë Key Features**:
- **CLIP-based embeddings** for text-image alignment
- **Unified vector space** for multimodal search
- **Visual context integration** in responses
- **Cross-modal retrieval** capabilities

**üìä Technical Stack**:
- `OpenAI CLIP` for multimodal embeddings
- `PIL` for image processing
- `sentence-transformers` for text embeddings
- `OpenAI GPT-4` for generation

### 3. Speculative RAG with News Outlet Summary

**üéØ Purpose**: Fast draft generation with verification for real-time news processing

```mermaid
graph TD
    A[News Articles] --> B[Parent Document Chunking]
    B --> C[FAISS Vector Store]
    C --> D[Parent Document Retriever]
    D --> E[Fast Draft Generation]
    E --> F[BART Summarizer]
    F --> G[Draft Summary]
    G --> H[Verification LLM]
    H --> I[Groq LLaMA 70B]
    I --> J[Final Verified Summary]
    
    style E fill:#e1f5fe
    style H fill:#f3e5f5
```

**üîë Key Features**:
- **Two-stage generation** (draft + verification)
- **Parent document retrieval** for context preservation
- **Fast local summarization** with BART
- **Large model verification** with Groq

**üìä Technical Stack**:
- `ParentDocumentRetriever` for hierarchical retrieval
- `BART` for fast summarization
- `Groq LLaMA 70B` for verification
- `NewsAPI` for real-time data

### 4. Corrective RAG

**üéØ Purpose**: Human-in-the-loop correction system for improved accuracy

```mermaid
graph TD
    A[User Query] --> B[Initial RAG Retrieval]
    B --> C[Gemma 9B Generation]
    C --> D[Initial Response]
    D --> E[User Feedback]
    E --> F[Corrective Loop]
    F --> G[Editor LLM]
    G --> H[LLaMA 70B Correction]
    H --> I[Improved Response]
    I --> J[Memory Storage]
    
    style E fill:#ffebee
    style G fill:#e8f5e8
```

**üîë Key Features**:
- **Dual LLM architecture** (generator + editor)
- **Human feedback integration**
- **Conversation memory** for context
- **Iterative improvement** process

**üìä Technical Stack**:
- `Gemma 9B` for initial generation
- `LLaMA 70B` for correction
- `ConversationBufferMemory` for context
- `Pydantic` for structured output

### 5. Self-RAG

**üéØ Purpose**: Self-improving RAG system with internal knowledge accumulation

```mermaid
graph TD
    A[Initial Query] --> B[External Knowledge Base]
    B --> C[RAG Generation]
    C --> D[Editor Refinement]
    D --> E[Internal Knowledge Store]
    E --> F[Next Query]
    F --> G{Sufficient Internal Knowledge?}
    G -->|Yes| H[Use Internal Store]
    G -->|No| B
    H --> I[Self-Sufficient Generation]
    I --> J[Continuous Learning]
    
    style E fill:#f0f4c3
    style J fill:#e1f5fe
```

**üîë Key Features**:
- **Self-sufficient knowledge building**
- **Dual retrieval system** (internal + external)
- **Continuous learning** from outputs
- **Adaptive retrieval** strategy

**üìä Technical Stack**:
- `Gemma 9B` for generation
- `LLaMA 70B` for editing
- `FAISS` for dual storage
- `MMR` for diverse retrieval

### 6. Agentic RAG

**üéØ Purpose**: Graph-based RAG workflow with integrated evaluation metrics

```mermaid
graph TD
    A[PDF Input] --> B[Setup Chain]
    B --> C[Load PDF Node]
    C --> D[Text Splitting Node]
    D --> E[Embedding Node]
    E --> F[LLM Config Node]
    F --> G[RAG Chain Start]
    G --> H[Retrieve Node]
    H --> I[Augmentation Node]
    I --> J[Dataset Setup Node]
    J --> K[RAGAS Evaluation Node]
    K --> L[Generation Node]
    L --> M[Final Response + Metrics]
    
    style B fill:#e3f2fd
    style K fill:#fff3e0
    style M fill:#e8f5e8
```

**üîë Key Features**:
- **Graph-based workflow** with LangGraph
- **Integrated RAGAS evaluation** for quality metrics
- **Modular node architecture**
- **State management** and checkpointing

**üìä Technical Stack**:
- `LangGraph` for workflow orchestration
- `RAGAS` for evaluation metrics
- `GPT-4` for high-quality generation
- `InMemorySaver` for state persistence

### 7. Adaptive RAG: Smart Retrieval from DOC or LLM

**üéØ Purpose**: Dynamically selects between document-based retrieval and LLM-generated content based on query complexity and context

```mermaid
graph TD
    A[User Query] --> B[Query Analyzer]
    B --> C{Complexity Assessment}
    C -->|Simple| D[Document Retrieval]
    C -->|Complex| E[LLM Content Generation]
    D --> F[FAISS Vector Store]
    E --> G[LLaMA 70B]
    F --> H[Retrieved Context]
    G --> H
    H --> I[Context Fusion]
    I --> J[Final Response Generation]
    J --> K[Response with Source Attribution]
    
    style B fill:#e1f5fe
    style C fill:#fff3e0
```

**üîë Key Features**:
- **Query complexity analysis** for dynamic source selection
- **Hybrid retrieval** combining document and LLM outputs
- **Context fusion** for coherent responses
- **Source attribution** for transparency

**üìä Technical Stack**:
- `LangChain` for query analysis
- `FAISS` for document retrieval
- `Groq LLaMA 70B` for content generation
- `sentence-transformers` for embeddings

### 8. Fusion RAG: MultiSource-MultiRetrieval-MultiLingual

**üéØ Purpose**: Combines multiple data sources, retrieval strategies, and multilingual processing for robust information retrieval

```mermaid
graph TD
    A[User Query] --> B[Language Detection]
    B --> C[Multilingual Preprocessing]
    C --> D[Multiple Data Sources]
    D --> E[Web Scraping]
    D --> F[PDF Documents]
    D --> G[Database Records]
    E --> H[Text Extraction]
    F --> H
    G --> H
    H --> I[Multi-Retrieval Strategies]
    I --> J[BM25 Retrieval]
    I --> K[Vector Search]
    I --> L[Semantic Search]
    J --> M[Combined Context]
    K --> M
    L --> M
    M --> N[Multilingual LLM]
    N --> O[Translated Response]
    
    style B fill:#e3f2fd
    style I fill:#fff3e0
```

**üîë Key Features**:
- **Multilingual support** for diverse queries
- **Multiple retrieval strategies** (BM25, vector, semantic)
- **Multisource integration** (web, PDFs, databases)
- **Context aggregation** for comprehensive responses

**üìä Technical Stack**:
- `langdetect` for language detection
- `transformers` for multilingual embeddings
- `FAISS` and `BM25` for retrieval
- `mBART` for multilingual generation

### 9. REFEED (Retrieval Feedback) HR Policy Fine-Tuning Without Training

**üéØ Purpose**: Enhances RAG performance for HR policy queries using retrieval feedback without retraining the model

```mermaid
graph TD
    A[HR Policy Query] --> B[Initial Retrieval]
    B --> C[FAISS Vector Store]
    C --> D[Retrieved Documents]
    D --> E[Relevance Feedback]
    E --> F[Query Refinement]
    F --> G[Refined Retrieval]
    G --> H[Policy-Specific Context]
    H --> I[LLaMA 70B Generation]
    I --> J[Response with Feedback Loop]
    J --> K[Feedback Storage]
    K --> F
    
    style E fill:#ffebee
    style K fill:#e8f5e8
```

**üîë Key Features**:
- **Relevance feedback loop** for query refinement
- **Policy-specific context** for HR applications
- **No model retraining** required
- **Iterative improvement** through feedback storage

**üìä Technical Stack**:
- `FAISS` for vector storage
- `Groq LLaMA 70B` for generation
- `Pydantic` for structured feedback
- `ConversationBufferMemory` for feedback storage

---

## üõ†Ô∏è Installation

### Prerequisites

```bash
Python 3.8+
pip package manager
```

### Quick Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/advanced-rag-experiments.git
cd advanced-rag-experiments

# Install dependencies
pip install -r requirements.txt
```

### Core Dependencies

```txt
langchain>=0.1.0
langchain-community>=0.0.20
langchain-groq>=0.0.5
langchain-openai>=0.0.5
faiss-cpu>=1.7.0
sentence-transformers>=2.2.0
transformers>=4.30.0
pdfplumber>=0.9.0
pymupdf>=1.23.0
openai>=1.0.0
groq>=0.4.0
ragas>=0.1.0
langgraph>=0.0.40
polars>=0.20.0
langdetect>=1.0.9
mbart>=0.1.0
```

---

## üìä Performance Comparison

| RAG Type | Processing Speed | Accuracy | Multimodal | Self-Improving | Complexity |
|----------|------------------|----------|------------|----------------|------------|
| **Metadata RAG** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚ùå | Medium |
| **MultiModal RAG** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚ùå | High |
| **Speculative RAG** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚ùå | ‚ùå | Medium |
| **Corrective RAG** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚úÖ | High |
| **Self-RAG** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚úÖ | High |
| **Agentic RAG** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚úÖ | Very High |
| **Adaptive RAG** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚úÖ | High |
| **Fusion RAG** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚ùå | Very High |
| **REFEED RAG** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚úÖ | High |

---

## üöÄ Quick Start

### 1. Basic RAG with Metadata

```python
from advanced_rag import MetadataRAG

# Initialize RAG system
rag = MetadataRAG(
    pdf_path="your_document.pdf",
    chunk_size=1000,
    embedding_model="all-MiniLM-L6-v2"
)

# Process document
rag.process_document()

# Query with source tracing
response = rag.query(
    "What is the main topic?",
    include_metadata=True
)
print(f"Answer: {response.answer}")
print(f"Sources: {response.sources}")
```

### 2. MultiModal RAG

```python
from advanced_rag import MultiModalRAG

# Initialize with CLIP embeddings
mmrag = MultiModalRAG(
    pdf_path="document_with_images.pdf",
    clip_model="openai/clip-vit-base-patch32"
)

# Process text and images
mmrag.process_multimodal_content()

# Query across modalities
response = mmrag.query(
    "Show me diagrams about network architecture",
    include_images=True
)
```

### 3. Agentic RAG with Evaluation

```python
from advanced_rag import AgenticRAG

# Initialize with graph workflow
arag = AgenticRAG(
    pdf_path="technical_document.pdf",
    evaluation_metrics=["faithfulness", "relevancy", "factual_correctness"]
)

# Run complete workflow
result = arag.run_workflow(
    question="Explain the main concepts",
    thread_id="session_1"
)

print(f"Answer: {result.answer}")
print(f"RAGAS Scores: {result.evaluation_scores}")
```

### 4. Adaptive RAG

```python
from advanced_rag import AdaptiveRAG

# Initialize adaptive RAG
arag = AdaptiveRAG(
    pdf_path="your_document.pdf",
    embedding_model="all-MiniLM-L6-v2",
    llm_model="groq/llama-70b"
)

# Process document and initialize LLM
arag.process_document()

# Query with adaptive retrieval
response = arag.query(
    "Explain complex concepts in simple terms",
    use_llm_if_complex=True
)
print(f"Answer: {response.answer}")
print(f"Source: {response.source_type}")
```

### 5. Fusion RAG

```python
from advanced_rag import FusionRAG

# Initialize fusion RAG
frag = FusionRAG(
    sources=["web", "pdf", "database"],
    embedding_model="all-MiniLM-L6-v2",
    multilingual_model="facebook/mbart-large-50"
)

# Process multiple sources
frag.process_sources()

# Query with multilingual support
response = frag.query(
    "Explain AI concepts in Spanish",
    language="es"
)
print(f"Answer: {response.answer}")
```

### 6. REFEED RAG

```python
from advanced_rag import RefeedRAG

# Initialize REFEED RAG
rrag = RefeedRAG(
    pdf_path="hr_policy_document.pdf",
    embedding_model="all-MiniLM-L6-v2",
    llm_model="groq/llama-70b"
)

# Process HR policy document
rrag.process_document()

# Query with feedback loop
response = rrag.query(
    "What is the leave policy?",
    feedback_enabled=True
)
print(f"Answer: {response.answer}")
print(f"Feedback Stored: {response.feedback_id}")
```

---

## üìà Future Roadmap

### üéØ Upcoming RAG Implementations

Based on the [25 Types of RAG](https://medium.com/projectpro/25-types-of-rag-which-one-fits-your-project-best-819d99b42d1a) analysis, we're planning to implement:

#### Phase 1 (Q2 2024)
- [x] **Hierarchical RAG** - Multi-level document understanding
- [x] **Fusion RAG** - Multiple retrieval strategy combination
- [ ] **Contextual RAG** - Enhanced context preservation

#### Phase 2 (Q3 2024)
- [ ] **Temporal RAG** - Time-aware information retrieval
- [ ] **Collaborative RAG** - Multi-agent collaboration
- [x] **Adaptive RAG** - Dynamic strategy selection

#### Phase 3 (Q4 2024)
- [ ] **Causal RAG** - Causal relationship understanding
- [ ] **Federated RAG** - Distributed knowledge systems
- [ ] **Quantum RAG** - Quantum-inspired retrieval

### üîß Technical Enhancements

- [ ] **Docker containerization** for easy deployment
- [ ] **REST API** for production integration
- [ ] **Benchmarking suite** for performance comparison
- [ ] **GUI interface** for non-technical users
- [ ] **Cloud deployment** guides (AWS, GCP, Azure)

---

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### üåü How to Contribute

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-rag`)
3. **Commit** your changes (`git commit -m 'Add amazing RAG implementation'`)
4. **Push** to the branch (`git push origin feature/amazing-rag`)
5. **Open** a Pull Request

### üìù Contribution Areas

- **New RAG implementations** from the roadmap
- **Performance optimizations** for existing systems
- **Documentation improvements** and tutorials
- **Bug fixes** and error handling
- **Testing** and quality assurance

---

## üìö Resources & References

### üìñ Research Papers
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511)
- [Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884)

### üîó Useful Links
- [LangChain Documentation](https://python.langchain.com/)
- [RAGAS Framework](https://docs.ragas.io/)
- [25 Types of RAG Analysis](https://medium.com/projectpro/25-types-of-rag-which-one-fits-your-project-best-819d99b42d1a)

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **LangChain** team for the excellent framework
- **Hugging Face** for transformer models
- **OpenAI** for CLIP and GPT models
- **Groq** for fast inference capabilities
- **RAGAS** team for evaluation metrics

---

## üìä Repository Statistics

![GitHub Stars](https://img.shields.io/github/stars/yourusername/advanced-rag-experiments?style=social)
![GitHub Forks](https://img.shields.io/github/forks/yourusername/advanced-rag-experiments?style=social)
![GitHub Issues](https://img.shields.io/github/issues/yourusername/advanced-rag-experiments)
![GitHub PRs](https://img.shields.io/github/issues-pr/yourusername/advanced-rag-experiments)

---

<div align="center">
  <h3>üöÄ Ready to explore the future of RAG?</h3>
  <p>Star this repository and join our growing community of AI researchers and practitioners!</p>
  
  [![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/yourusername/advanced-rag-experiments)
  [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/yourprofile)
  [![Twitter](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/yourhandle)
</div>

---

*Last updated: July 2025*