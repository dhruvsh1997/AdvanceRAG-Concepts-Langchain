# -*- coding: utf-8 -*-
"""Fusion RAG-MultiSource-MultiRetrival-MultiLingual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdmSTu6WPJ_Z-lIaFInfYUh8IS3KB1PS

###What is Fusion RAG?
- Fusion RAG stands for “Many Sources, One Answer.” It retrieves context from multiple retrievers (e.g., different documents or data types), then combines the retrieved results before sending them to an LLM for generation.
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain sentence-transformers faiss-cpu pypdf groq langchain-community langchain-groq

# ================== IMPORTS ==================
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.runnables import (
    RunnableMap, RunnableLambda, RunnablePassthrough, RunnableSequence
)
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain_groq import ChatGroq
from langchain_core.documents import Document
from google.colab import userdata

# ================== LOAD LEGAL PDF ==================
loader = PyPDFLoader("/content/BharatKaSamvidhan.pdf")  # Replace with actual path
legal_docs = loader.load_and_split()

# ================== SIMULATED MULTILINGUAL DOCS ==================
multilingual_docs = [
    Document(page_content="नमस्ते! आप कानूनी सहायता हिंदी में प्राप्त कर सकते हैं। कृपया अपना प्रश्न पूछें।"),
    Document(page_content="You can ask questions in English or Hindi. The assistant will understand both."),
    Document(page_content="This assistant provides Indian constitutional legal guidance in multiple languages.")
]

# ================== TEXT SPLITTER ==================
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
legal_split = splitter.split_documents(legal_docs)
multi_split = splitter.split_documents(multilingual_docs)

# ================== EMBEDDING MODELS ==================
legal_embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
hindi_embedder = HuggingFaceEmbeddings(model_name="AkshitaS/bhasha-embed-v0")

# ================== VECTOR STORES ==================
legal_store = FAISS.from_documents(legal_split, legal_embedder)
multi_store = FAISS.from_documents(multi_split, hindi_embedder)

# ================== RETRIEVERS ==================
legal_retriever = legal_store.as_retriever(search_kwargs={"k": 3})
multi_retriever = multi_store.as_retriever(search_kwargs={"k": 2})

# ================== LLM SETUP ==================
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get("GROQ_API_KEY")
)
llm

# ================== MAIN PROMPT ==================
rag_prompt = PromptTemplate.from_template(
    "Using the following context, answer the question:\n\n{context}\n\nQuestion: {question}"
)

# ================== QUERY REWRITE PROMPT ==================
query_rewrite_prompt = ChatPromptTemplate.from_template(
    "You are a multilingual query optimizer. Convert the user query to English and make it more specific for a legal RAG system.\n\n"
    "Original Query: {original_query}\n\nRewritten Query:"
)

# ================== QUERY BUILDER REWRITER CHAIN ==================
query_rewriter_chain = query_rewrite_prompt | llm | StrOutputParser()

# ================== FUSION RETRIEVER ==================
fusion_retriever = RunnableLambda(
    lambda x: legal_retriever.get_relevant_documents(x["rewritten_query"]) +
              multi_retriever.get_relevant_documents(x["rewritten_query"])
)
fusion_retriever

# ================== MEMORY ==================
memory = ConversationBufferMemory(return_messages=True)
conversation = ConversationChain(llm=llm, memory=memory)

# ================== FINAL CHAIN ==================
fusion_rag_chain = (
    # Step 1: Rewrite query
    RunnableMap({
        "original_query": RunnablePassthrough()
    }) |
    RunnableMap({
        "rewritten_query": query_rewriter_chain
    }) |
    # Step 2: Fusion retrieval using rewritten query
    RunnableMap({
        "context": fusion_retriever,
        "question": lambda x: x["rewritten_query"]
    }) |
    # Step 3: Format prompt + generate
    rag_prompt |
    llm |
    StrOutputParser()
)
fusion_rag_chain

# ================== RUN QUERY ==================
user_query = "क्या अनुच्छेद 21 जीवन के अधिकार को कवर करता है?"

# Add to memory (optional)
memory.chat_memory.add_user_message(user_query)

# Run chain
response = fusion_rag_chain.invoke(user_query)

# Save to memory
memory.chat_memory.add_ai_message(response)

# Show response
print("Answer:\n", response)



"""Legal Retriever	Extracts relevant text from the Constitution<br>
Multilingual Retriever	Supports Hindi/English queries<br>
Query Rewriter	Converts user queries to better structured retrieval prompts<br>
Fusion RAG	Combines both sources for richer answers<br>
Memory	Tracks prior Q&A for follow-up support<br>
"""

