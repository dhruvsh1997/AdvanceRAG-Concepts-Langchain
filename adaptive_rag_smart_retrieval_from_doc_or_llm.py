# -*- coding: utf-8 -*-
"""Adaptive RAG: Smart Retrieval from DOC or LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bb1vU5h71Ul2pkLp1afikiGqFMCCguKB

###What is Adaptive RAG?
Adaptive RAG is an enhancement over traditional RAG. It adds intelligence in deciding:

- When to retrieve (for complex or ambiguous queries).

- When not to retrieve (if the LLM is confident enough).

- Optionally uses honesty probes or confidence thresholds to balance between internal LLM memory and external vectorstore knowledge.
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain langchain-community langchain-core langchain-groq sentence-transformers faiss-cpu pypdf

# ===================== IMPORTS =====================
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import (
    RunnableLambda, RunnableMap, RunnablePassthrough
)
from langchain_core.output_parsers import StrOutputParser

# ===================== MOUNT GOOGLE DRIVE =====================
from google.colab import drive
# drive.mount('/content/drive')

# Define your folder path with PDFs
pdf_folder = "/content/primay-health-centres.pdf"
pdf_paths = [pdf_folder]#os.path.join(pdf_folder, file) for file in os.listdir(pdf_folder) if file.endswith(".pdf")]

# ===================== LOAD AND SPLIT =====================
all_docs = []
for pdf in pdf_paths:
    loader = PyPDFLoader(pdf)
    docs = loader.load_and_split()
    all_docs.extend(docs)

# ===================== SPLIT INTO CHUNKS =====================
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(all_docs)
print(f"Total chunks: {len(docs)}")

# ===================== EMBEDDINGS + VECTORSTORE =====================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding_model)
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5}, lambda_mult=0.3)

# ===================== DEFINE LLM =====================
from google.colab import userdata

llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",  # matches Groq deployment
    api_key=userdata.get("GROQ_API_KEY")
)
llm

# ===================== PROMPTS =====================
query_rewriter_prompt = ChatPromptTemplate.from_template(
    "You are a helpful AI assistant. Rephrase the following question to be more clear and specific for retrieval:\n\n"
    "Original: {question}\n\nRephrased:"
)

context_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a clinical assistant AI. Use the context to answer medical questions clearly."),
    ("human", "Context:\n{context}\n\nQuestion: {question}")
])

# ===================== QUERY REWRITER CHAIN =====================
query_rewriter_chain = query_rewriter_prompt | llm | StrOutputParser()

# ===================== CONFIDENCE SCORER =====================
confidence_prompt = ChatPromptTemplate.from_template(
    "How confident are you in answering this question from your own knowledge, without external help?\n"
    "Question: {question}\nRespond with one word only: High or Low."
)

confidence_chain = confidence_prompt | llm | StrOutputParser()

def should_retrieve_based_on_confidence(question: str) -> bool:
    confidence = confidence_chain.invoke({"question": question})
    return "low" in confidence.lower()

# ===================== RETRIEVER WRAPPER (WITH SOURCE FLAG) =====================
def adaptive_retriever(inputs):
    question = inputs["question"]
    if should_retrieve_based_on_confidence(question):
        improved_query = query_rewriter_chain.invoke({"question": question})
        docs = retriever.get_relevant_documents(improved_query)
        context = "\n\n".join([doc.page_content for doc in docs])
        source = "retrieved"
    else:
        context = ""
        source = "llm_only"
    return {"context": context, "question": question, "source": source}

# ===================== FINAL RAG CHAIN WITH SOURCE METADATA =====================
def include_source_in_output(output, source):
    return {
        "answer": output.content,
        "source": source
    }

rag_chain = (
    RunnableLambda(adaptive_retriever)
    | RunnableMap({
        "output": context_prompt | llm,
        "source": lambda x: x["source"]  # propagate source flag
    })
    | RunnableLambda(lambda x: include_source_in_output(x["output"], x["source"]))
)
rag_chain

# ===================== RUN THE ADAPTIVE RAG (DOC) =====================
question = "What proforma for Facility Survey for PHC on IPHS?"
response = rag_chain.invoke({"question": question})
print(f"Answer:\n{response['answer']}\n\nSource: {response['source']}")

# ===================== RUN THE ADAPTIVE RAG (LLM) =====================
question = "Who is Owner of BMW?"
response = rag_chain.invoke({"question": question})
print(f"Answer:\n{response['answer']}\n\nSource: {response['source']}")





