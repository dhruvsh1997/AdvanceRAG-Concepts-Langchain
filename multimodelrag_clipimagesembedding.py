# -*- coding: utf-8 -*-
"""MultiModelRAG-CLIPImagesEmbedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14eJTDYyCFtBXGrjuSTZAyiBdBuNzLW7U
"""

# Install core dependencies
!pip install langchain langchain_community langchain-openai openai sentence-transformers faiss-cpu pymupdf Pillow

# Commented out IPython magic to ensure Python compatibility.
# Replace with your actual OpenAI API key
import os
from google.colab import userdata
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/Colab Notebooks/AdvanceRAG Problems
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY_')

# PDF processing
import fitz  # PyMuPDF
# Image handling
from PIL import Image
# Embedding models
from sentence_transformers import SentenceTransformer
from transformers import CLIPProcessor, CLIPModel
# Vector DB
import faiss
import numpy as np
# LangChain components
from langchain.schema import Document
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import MultiVectorRetriever
import uuid
import os

pdf_path = "dataPDF/OSI-TCPIP.pdf"
doc = fitz.open(pdf_path)
print(f"Loaded PDF with {len(doc)} pages.")

# Create a folder for extracted images
os.makedirs("MMDRAGimages", exist_ok=True)

pages_data = []

for page_number in range(len(doc)):
    page = doc[page_number]
    text = page.get_text()

    # Extract images on this page
    image_list = []
    for img_index, img in enumerate(page.get_images(full=True)):
        xref = img[0]
        base_image = doc.extract_image(xref)
        image_bytes = base_image["image"]
        image_ext = base_image["ext"]
        image_filename = f"MMDRAGimages/page{page_number+1}_img{img_index+1}.{image_ext}"

        with open(image_filename, "wb") as f:
            f.write(image_bytes)

        image_list.append({
            "image_path": image_filename,
            "caption": f"Image extracted from page {page_number+1}"
        })

    pages_data.append({
        "page_number": page_number + 1,
        "text": text,
        "images": image_list
    })

print(f"Extracted text and images from {len(pages_data)} pages.")

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

all_documents = []

for page in pages_data:
    chunks = text_splitter.split_text(page["text"])
    for i, chunk in enumerate(chunks):
        all_documents.append({
            "id": str(uuid.uuid4()),
            "type": "text",
            "content": chunk,
            "page_number": page["page_number"]
        })

print(f"Total text chunks: {len(all_documents)}")

# Load CLIP model and processor
# Use CLIP text encoder instead of all-MiniLM-L6-v2
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

clip_model.eval()

# Generate text embeddings using CLIP
text_embeddings = []

for doc in all_documents:
    # Truncate the text content to the maximum sequence length for CLIP
    truncated_content = doc["content"][:77]  # Truncate to 77 characters as a proxy for tokens
    inputs = clip_processor(text=[truncated_content], return_tensors="pt", padding=True)
    outputs = clip_model.get_text_features(**inputs)
    embedding = outputs[0].detach().numpy()
    embedding = embedding / np.linalg.norm(embedding)  # normalize

    text_embeddings.append({
        "id": doc["id"],
        "embedding": embedding.tolist(),
        "metadata": doc
    })

print(f"Generated embeddings for {len(text_embeddings)} text chunks.")



# Load CLIP model and processor
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

image_embeddings = []

for page in pages_data:
    for image in page["images"]:
        pil_image = Image.open(image["image_path"]).convert("RGB")
        inputs = clip_processor(images=pil_image, return_tensors="pt")
        outputs = clip_model.get_image_features(**inputs)
        embedding = outputs[0].detach().numpy()
        embedding = embedding / np.linalg.norm(embedding)  # normalize

        image_embeddings.append({
            "id": str(uuid.uuid4()),
            "embedding": embedding.tolist(),
            "metadata": {
                "type": "image",
                "image_path": image["image_path"],
                "caption": image["caption"],
                "page_number": page["page_number"]
            }
        })

print(f"Generated embeddings for {len(image_embeddings)} images.")

# Combine text and image embeddings
all_vectors = text_embeddings + image_embeddings

dimension = len(all_vectors[0]["embedding"])
index = faiss.IndexFlatIP(dimension)
# Create FAISS index
vectors_np = np.array([vec["embedding"] for vec in all_vectors]).astype('float32')
index.add(vectors_np)

metadata_list = [vec["metadata"] for vec in all_vectors]

print(f"FAISS index built with {index.ntotal} vectors.")

import polars as pl

# Create a list of dictionaries from the combined data
# Each dictionary should contain the metadata and the embedding
vector_db_data = []
for i, vec in enumerate(all_vectors):
    data_row = vec["metadata"].copy() # Start with metadata
    data_row["embedding"] = vec["embedding"] # Add the embedding
    vector_db_data.append(data_row)

# Create a Polars DataFrame
vectordb_df = pl.DataFrame(vector_db_data)

# Display the DataFrame
vectordb_df

from sklearn.metrics.pairwise import cosine_similarity

def multimodal_retriever(query, top_k=5, lambda_mult=0.5):
    # Embed query with CLIP text encoder
    inputs = clip_processor(text=[query], return_tensors="pt", padding=True)
    outputs = clip_model.get_text_features(**inputs)
    query_embedding = outputs[0].detach().numpy()
    query_embedding = query_embedding / np.linalg.norm(query_embedding)

    # Compute cosine similarities
    similarities = cosine_similarity([query_embedding], vectors_np)[0]

    # Get top_k indices
    top_k_idx = similarities.argsort()[-top_k:][::-1]

    results = []
    for idx in top_k_idx:
        results.append({
            "similarity": similarities[idx],
            "metadata": metadata_list[idx]
        })
    return results

# Create ChatPromptTemplate
prompt_template = ChatPromptTemplate.from_template("""
You are a helpful assistant.
Use the context below to answer the question.
Include any referenced images in your response by mentioning the file path.

Context:
{context}

Question:
{question}
""")

llm = ChatOpenAI(model_name="gpt-4o-mini")

output_parser = StrOutputParser()

def answer_query(query):
    retrieved = multimodal_retriever(query, top_k=5)
    # breakpoint()
    context = ""
    for item in retrieved:
        meta = item["metadata"]
        if meta["type"] == "text":
            context += f"[Text] {meta['content']}\n"
        elif meta["type"] == "image":
            context += f"[Image] {meta['caption']} (File: {meta['image_path']})\n"

    prompt = prompt_template.format_messages(
        context=context,
        question=query
    )

    response = llm(prompt)
    answer = output_parser.parse(response)
    return answer, retrieved

import matplotlib.pyplot as plt
question = "Explain Diagrammatic Comparison between OSI Reference Model and TCP/IP."
answer, retrieved = answer_query(question)

print("Assistant Response:\n")
print(answer)

# After your LLM answer
print("\nRelated Images:")

for item in retrieved:
    meta = item["metadata"]

    # If it's an image vector directly, just display it
    if meta["type"] == "image":
        image_path = meta["image_path"]
        img = Image.open(image_path)
        plt.figure(figsize=(6, 6))
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"Direct match: {image_path}")
        plt.show()

    # If it's a text chunk, find any image vector on the same page
    elif meta["type"] == "text":
        page_num = meta["page_number"]

        # Search metadata_list for images with the same page
        related_images = [
            m for m in metadata_list
            if m["type"] == "image" and m["page_number"] == page_num
        ]

        for img_meta in related_images:
            image_path = img_meta["image_path"]
            img = Image.open(image_path)
            plt.figure(figsize=(6, 6))
            plt.imshow(img)
            plt.axis('off')
            plt.title(f"Image from page {page_num}: {image_path}")
            plt.show()

retrieved

import matplotlib.pyplot as plt

# Filter retrieved image metadata
retrieved_images = [
    item["metadata"] for item in retrieved if item["metadata"]["type"] == "image"
]

print("\nRelated Images:",retrieved_images)

# Show each image in a grid using Matplotlib
for img_meta in retrieved_images:
    print(img_meta["image_path"])
    img = Image.open(img_meta["image_path"])

    plt.figure(figsize=(5, 5))
    plt.imshow(img)
    plt.title(f"Page: {img_meta['page_number']} - {img_meta['caption']}")
    plt.axis('off')
    plt.show()











text_metadata_list = [vec["metadata"] for vec in text_embeddings]
image_metadata_list = [vec["metadata"] for vec in image_embeddings]
def multimodal_retriever(query, top_k=5):
    # Embed query for text
    query_text_embedding = text_model.encode(query)
    query_text_embedding = query_text_embedding / np.linalg.norm(query_text_embedding)

    # Embed query for image
    inputs = clip_processor(text=[query], return_tensors="pt")
    query_image_embedding = clip_model.get_text_features(**inputs).detach().numpy()[0]
    query_image_embedding = query_image_embedding / np.linalg.norm(query_image_embedding)

    # Search text index
    _, text_idx = text_index.search(np.array([query_text_embedding]).astype('float32'), top_k)

    # Search image index
    _, image_idx = image_index.search(np.array([query_image_embedding]).astype('float32'), top_k)

    # Combine results
    results = []
    for idx in text_idx[0]:
        results.append({"similarity": None, "metadata": text_metadata_list[idx]})
    for idx in image_idx[0]:
        results.append({"similarity": None, "metadata": image_metadata_list[idx]})

    return results