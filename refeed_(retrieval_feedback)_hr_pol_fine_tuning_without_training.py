# -*- coding: utf-8 -*-
"""REFEED_(Retrieval_Feedback)_HR_Pol_Fine_Tuning_Without_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PBNCyKLw8-SPshhEymKjz644QtFMme4W

###What is REFEED RAG?
REFEED RAG improves results without retraining by:

- Observing which retrieved results are relevant (clicks, corrections, thumbs up/down, etc.)

- Using this feedback to re-rank future retrievals.

- Sometimes boosting previously successful chunks when similar questions appear again.

- We’re not changing the model, but how you retrieve and rank context.

Intelligent Interview Assistant for HR
* Uses internal documents (policies, evaluation rubrics, guides).

* Retrieves info to assist in interviews.

* Learns over time from HR’s feedback (e.g., “This is outdated”).

* Improves future retrievals by re-ranking based on history.
"""

# ================== Install Dependencies ==================
!pip install -q langchain langchain-core langchain-community langchain-groq sentence-transformers faiss-cpu pypdf

# ================== Imports ==================
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableMap
from langchain_core.output_parsers import StrOutputParser
import os
import json

# ================== Load HR Docs ==================
from google.colab import drive
# drive.mount('/content/drive')

# Load PDFs from HR_Policies folder
hr_folder = "/content/Human-Resources-Policy.pdf"
pdfs = [hr_folder]#os.path.join(hr_folder, file) for file in os.listdir(hr_folder) if file.endswith(".pdf")]

docs = []
for pdf in pdfs:
    loader = PyPDFLoader(pdf)
    docs.extend(loader.load_and_split())

# Split into chunks
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(docs)

print(f"Total HR Chunks: {len(chunks)}")

# ================== Embed and Store in FAISS ==================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunks, embedding_model)
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})

# ================== LLM SETUP ==================
from google.colab import userdata

llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get("GROQ_API_KEY")
)
llm

# ================== Prompt Template ==================
hr_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an intelligent HR assistant. Use company policy context to answer interview-related questions."),
    ("human", "Context:\n{context}\n\nQuestion: {question}")
])
hr_prompt

# ================== REFEED Feedback Store (JSON-Based) ==================
# Store feedback as a local dict or JSON file (in real-world: use vectorstore metadata or a DB)
feedback_log_path = "/content/hr_feedback_log.json"

# Initialize feedback log if not exists
if not os.path.exists(feedback_log_path):
    with open(feedback_log_path, "w") as f:
        json.dump({}, f)

# Load feedback history
def load_feedback():
    with open(feedback_log_path, "r") as f:
        return json.load(f)

# Save updated feedback
def save_feedback(feedback_data):
    with open(feedback_log_path, "w") as f:
        json.dump(feedback_data, f, indent=2)

# ================== Retrieval + Reranking Logic with Feedback Boost ==================
def rerank_documents_with_feedback(question, docs):
    feedback = load_feedback()
    reranked = []

    for doc in docs:
        text = doc.page_content
        score = 1.0

        # Boost if this doc had positive feedback before for similar question
        for past_q, entries in feedback.items():
            if past_q.lower() in question.lower():  # Simple match, can use semantic sim
                for e in entries:
                    if e["text"] in text:
                        score += e["score"]  # 1 for positive, -1 for negative

        reranked.append((score, doc))

    reranked.sort(key=lambda x: x[0], reverse=True)
    return [doc for score, doc in reranked]

# ================== Build the REFEED RAG Chain ==================
def retrieve_with_feedback(inputs):
    question = inputs["question"]
    retrieved = retriever.get_relevant_documents(question)
    reranked_docs = rerank_documents_with_feedback(question, retrieved)
    context = "\n\n".join([doc.page_content for doc in reranked_docs])
    return {"context": context, "question": question}

refeed_chain = (
    RunnableLambda(retrieve_with_feedback)
    | hr_prompt
    | llm
)
refeed_chain

# ================== Ask Questions and Get Answers ==================
question = "What is our policy for remote work during probation?"
response = refeed_chain.invoke({"question": question})
print(response.content)

# ================== Collect Feedback on Response ==================
# Sample feedback UI in notebook
user_feedback = input("Was the answer correct and useful? (yes/no): ").strip().lower()

if user_feedback in ["yes", "no"]:
    feedback = load_feedback()
    past = feedback.get(question, [])

    # Store top context snippet
    doc_snippet = response.content[:200]  # Top part of generated answer
    past.append({
        "text": doc_snippet,
        "score": 1 if user_feedback == "yes" else -1
    })

    feedback[question] = past
    save_feedback(feedback)
    print("Feedback recorded. Retrieval will improve next time.")



"""Feature	Implemented<br>
Dynamic retrieval	<br>
Post-retrieval reranking	<br>
Feedback collection	<br>
Feedback persistence	<br>
Context-aware reweighting	<br>
"""







