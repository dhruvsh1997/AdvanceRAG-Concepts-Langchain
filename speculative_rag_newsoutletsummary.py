# -*- coding: utf-8 -*-
"""Speculative_RAG-NewsOutletSummary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fWmSIpEOddQSR-ZDsJTGeMmg9II-LyvP
"""

# Install essentials
!pip install langchain-groq faiss-cpu transformers sentence-transformers langchain_community

# Core LangChain & embeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import ParentDocumentRetriever
from langchain_core.documents import Document
# Groq LLM
from langchain_groq import ChatGroq
# LangChain core for chains and prompts
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableSequence, RunnableParallel
# Hugging Face local summarizer
from transformers import pipeline
# Other
import os
import requests
import numpy as np

# Provide your Groq API key only
from google.colab import userdata
GROQ_API_KEY = userdata.get('GROQ_API_KEY')
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

# Example using NewsAPI.org (mock if you want)
news_api_key = userdata.get('NEWS_API_KEY')
url = f"https://newsapi.org/v2/top-headlines?country=us&apiKey={news_api_key}"

response = requests.get(url)
data = response.json()
articles = data["articles"]

# Turn each article into a parent Document
parent_docs = []
for i, article in enumerate(articles):
    text = f"{article['title']}\n{article['description']}\n{article['content']}"
    parent_docs.append(
        Document(page_content=text, metadata={"id": str(i)})
    )

print(f"Loaded {len(parent_docs)} parent articles.")

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.storage import InMemoryStore
# Use RecursiveCharacterTextSplitter to chunk parent docs into children
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# Create embeddings
embedder = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store in FAISS using from_texts
vectorstore = FAISS.from_texts(["dummy"], embedding=embedder)
docstore = InMemoryStore()  # Stores parent docs

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=docstore,
    child_splitter=text_splitter,
    parent_splitter=None,  # no parent split, original docs are used
)
retriever

import polars as pl

# Create a list of dictionaries from the combined data
# Each dictionary should contain the metadata and the embedding
vector_db_data = []
for i, vec in enumerate(all_vectors):
    data_row = vec["metadata"].copy() # Start with metadata
    data_row["embedding"] = vec["embedding"] # Add the embedding
    vector_db_data.append(data_row)

# Create a Polars DataFrame
vectordb_df = pl.DataFrame(vector_db_data)

# Display the DataFrame
vectordb_df

# Add parent docs: the retriever will chunk them and index child chunks
retriever.add_documents(parent_docs)
print("ParentDocumentRetriever ready.")

# Small LLM: BART summarizer for quick draft summaries
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def draft_summary(inputs):
    """
    Speculative RAG: Fast draft stage.
    Uses summarizer to quickly generate a rough summary.
    """
    context = inputs["context"]
    combined_text = " ".join([doc.page_content for doc in context])
    result = summarizer(combined_text, max_length=100, min_length=30, do_sample=False)
    return result[0]["summary_text"]

# Big LLM for final verification and refinement
big_llm = ChatGroq(
    groq_api_key=GROQ_API_KEY,
    model_name="llama-3.3-70b-versatile"
)

# Prompt for the verifying big LLM using ChatPromptTemplate
verify_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(
        "You are a professional news editor and fact-checker. "
        "Ensure the final summary is accurate, clear, and aligns with the facts. "
        "No hallucinations. Keep it concise."
    ),
    HumanMessagePromptTemplate.from_template(
        "Question: {question}\n\n"
        "Relevant News Context:\n{context}\n\n"
        "Draft Summary:\n{draft}\n\n"
        "Please rewrite or approve this draft, under 100 words."
    )
])

# === Speculative RAG Workflow ===

# Retrieve relevant parent docs via ParentDocumentRetriever
def retrieve_parent_docs(inputs):
    """
    Retrieves parent docs relevant to the question.
    """
    return retriever.invoke(inputs["question"])

# Fast draft: uses local summarizer
draft_chain = RunnableLambda(lambda x: {"context": retrieve_parent_docs(x)})
draft_summary_chain = RunnableLambda(draft_summary)

def speculative_draft(inputs):
    context = retriever.invoke(inputs["question"])
    combined_text = " ".join([doc.page_content for doc in context])
    result = summarizer(combined_text, max_length=100, min_length=30, do_sample=False)
    return result[0]["summary_text"]

# Verify & Refine: uses Groq Llama with system/human prompt
verify_chain = (
    RunnableParallel({
        "context": retrieve_parent_docs,
        "draft": RunnableLambda(speculative_draft),
        "question": lambda x: x["question"]
    })
    | verify_prompt
    | big_llm
)

# Final Speculative RAG pipeline
speculative_rag = RunnableSequence(verify_chain)
print("Speculative RAG Chain with ParentDocumentRetriever ready!")

query = {"question": "Summarize the latest updates in US politics."}

result = speculative_rag.invoke(query)

print("\nFinal Verified Summary:")
print(result.content)

query = {"question": "Summarize the latest updates in NVIDIA Stocks."}

result = speculative_rag.invoke(query)

print("\nFinal Verified Summary:")
print(result.content)

query = {"question": "Summarize the latest updates in Business."}

result = speculative_rag.invoke(query)

print("\nFinal Verified Summary:")
print(result.content)

